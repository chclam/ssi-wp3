{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables from .env file for project\n",
    "dotenv_path = Path('../.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.getenv(\"OUTPUT_DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_df = pd.read_parquet(os.path.join(data_directory, 'ssi_omzet_eans_coicops_lidl_2018_202308.parquet'), engine=\"pyarrow\")\n",
    "lidl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "feature_extractor_dict = {'CountVect': CountVectorizer(analyzer='word', token_pattern=r'\\w{2,}', max_features=5000),\n",
    "                               'TFIDF_word': TfidfVectorizer(analyzer='word', token_pattern=r'\\w{2,}', max_features=5000),\n",
    "                               'TFIDF_char': TfidfVectorizer(analyzer='char', token_pattern=r'\\w{2,}', ngram_range=(2,3), max_features=5000),\n",
    "                               'TFIDF_char34': TfidfVectorizer(analyzer='char', token_pattern=r'\\w{2,}', ngram_range=(3,4), max_features=5000),\n",
    "                               'Count_char': CountVectorizer(analyzer='char', token_pattern=r'\\w{2,}', max_features=5000)\n",
    "                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_sample_df = lidl_df.sample(1000).reset_index(drop=True)\n",
    "lidl_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def tsne_plot(dataframe: pd.DataFrame, feature_extractor, plot_title: str, text_column: str, label_column: str):\n",
    "    tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(dataframe[label_column].values)\n",
    "\n",
    "    features = feature_extractor.fit_transform(dataframe[text_column])\n",
    "    embedded_features = tsne.fit_transform(features)    \n",
    "    plt.scatter(embedded_features[:,0], embedded_features[:, 1], c=y_true)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for i, (name, extractor) in enumerate(feature_extractor_dict.items()):\n",
    "        #if i > 1:\n",
    "        #    break\n",
    "        tsne_plot(lidl_sample_df, extractor, name, \"ean_name\", \"coicop_division\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_sample_df[\"ean_name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(lidl_sample_df[\"ean_name\"][1])\n",
    "doc.vector.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(lidl_sample_df[\"coicop_division\"].values)\n",
    "\n",
    "features = np.array([nlp(ean_name).vector for ean_name in lidl_sample_df[\"ean_name\"]])\n",
    "embedded_features = tsne.fit_transform(features)    \n",
    "plt.scatter(embedded_features[:,0], embedded_features[:, 1], c=y_true)\n",
    "plt.title(\"word embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_md = spacy.load(\"nl_core_news_md\")\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(lidl_sample_df[\"coicop_division\"].values)\n",
    "\n",
    "features = np.array([nlp_md(ean_name).vector for ean_name in lidl_sample_df[\"ean_name\"]])\n",
    "embedded_features = tsne.fit_transform(features)    \n",
    "plt.scatter(embedded_features[:,0], embedded_features[:, 1], c=y_true)\n",
    "plt.title(\"word embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(lidl_sample_df[\"coicop_division\"].values)\n",
    "\n",
    "features = np.array([nlp_lg(ean_name).vector for ean_name in lidl_sample_df[\"ean_name\"]])\n",
    "embedded_features = tsne.fit_transform(features)    \n",
    "plt.scatter(embedded_features[:,0], embedded_features[:, 1], c=y_true)\n",
    "plt.title(\"word embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_features_and_labels(nlp_model, dataframe: pd.DataFrame, text_column: str = \"ean_name\", label_column: str = \"coicop_division\") -> Tuple[LabelEncoder, pd.DataFrame]:\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(dataframe[label_column].values)\n",
    "\n",
    "    features = [nlp_model(ean_name).vector for ean_name in tqdm(dataframe[text_column])]\n",
    "    return label_encoder, pd.DataFrame({ \n",
    "        \"features\": features,\n",
    "        \"original_label\": dataframe[label_column],\n",
    "        \"label\": y_true\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lidl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lidl_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = lidl_df.month.unique().tolist()\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_month = months[12]\n",
    "selected_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(list({month[:4] for month in months}))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_year =  years[1]\n",
    "lidl_train_sample = lidl_df[lidl_df.month.str[:4] == selected_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_filename = os.path.join(data_directory, \"ssi_lidl_features_spacy_nl_md.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_features_filename):\n",
    "    label_encoder, features_df = extract_features_and_labels(nlp_md, lidl_train_sample)\n",
    "    features_df.to_parquet(train_features_filename, engine=\"pyarrow\")\n",
    "else:\n",
    "    features_df = pd.read_parquet(train_features_filename, engine=\"pyarrow\")    \n",
    "    \n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_data, test_data = train_test_split(features_df, test_size=0.2, stratify=features_df.label)\n",
    "train_val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "lr_model = logistic_regression.fit(train_val_data.features.values.tolist(), train_val_data.label.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = lr_model.predict(test_data.features.values.tolist())\n",
    "\n",
    "print(classification_report(test_data.label.values, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_md_features_name = os.path.join(data_directory, \"ssi_lidl_all_features_spacy_nl_md.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_test = lidl_df.copy()\n",
    "\n",
    "if not os.path.exists(all_md_features_name):\n",
    "    lidl_test[\"label\"] = label_encoder.transform(lidl_test[\"coicop_division\"].values)\n",
    "    lidl_test[\"features\"] = [nlp_md(ean_name).vector for ean_name in tqdm(lidl_test[\"ean_name\"], position=0, leave=True)]\n",
    "    lidl_test.to_parquet(all_md_features_name, engine=\"pyarrow\")\n",
    "else:\n",
    "    lidl_test = pd.read_parquet(all_md_features_name, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from typing import List\n",
    "\n",
    "def scores_per_label(model, label_encoder, dataframe: pd.DataFrame, years: List[str], label_column: str) -> pd.DataFrame:\n",
    "    year_array = []\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for year in years:\n",
    "        year_df = dataframe[dataframe.month.str[:4] == year]\n",
    "        year_labels = label_encoder.transform(year_df[label_column].values)\n",
    "        year_features = year_df.features.values.tolist()\n",
    "        y_pred = model.predict(year_features)\n",
    "    \n",
    "        year_array.extend([year for _ in range(len(year_labels))])\n",
    "        labels.extend(year_labels.tolist())\n",
    "        predictions.extend(y_pred.tolist())\n",
    "        f1_scores.append(f1_score(year_labels, y_pred, average=\"weighted\"))\n",
    "    \n",
    "    year_results_df = pd.DataFrame({\n",
    "        \"year\": year_array,\n",
    "        \"label\": labels,\n",
    "        \"prediction\": predictions\n",
    "    })\n",
    "    return f1_scores, year_results_df\n",
    "\n",
    "test_years = sorted(list({month[:4] for month in lidl_test.month}))  \n",
    "f1_scores, year_results_df = scores_per_label(lr_model, label_encoder, lidl_test, test_years, \"coicop_division\")\n",
    "plt.plot(test_years, f1_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_results_df = year_results_df.set_index([\"year\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_f1_score(row: pd.Series) -> float:\n",
    "    return f1_score(row.index.get_level_values(1), row, average=\"weighted\")\n",
    "    \n",
    "f1_scores_per_group = year_results_df.groupby(by=[\"year\", \"label\"]).agg(agg_f1_score)\n",
    "f1_scores_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_df = f1_scores_per_group.reset_index()\n",
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = f1_scores_df.year.unique()\n",
    "labels = f1_scores_df.label.unique()\n",
    "\n",
    "f1_scores_per_group.unstack(level=1\n",
    "                           ).plot(subplots=True, rot=90, figsize=(10, 10), layout=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"coicop_group\"\n",
    "\n",
    "cg_label_encoder, cg_features_df = extract_features_and_labels(nlp_md, lidl_train_sample, label_column=label_column)\n",
    "cg_train_val_data, cg_test_data = train_test_split(cg_features_df, test_size=0.2, stratify=cg_features_df.label)\n",
    "\n",
    "cg_lr_model = logistic_regression.fit(cg_train_val_data.features.values.tolist(), cg_train_val_data.label.values.tolist())\n",
    "cg_y_pred = cg_lr_model.predict(cg_test_data.features.values.tolist())\n",
    "\n",
    "print(classification_report(cg_test_data.label.values, cg_y_pred, target_names=cg_label_encoder.classes_))\n",
    "\n",
    "#year_results_df = scores_per_label(lr_model, cg_label_encoder, lidl_test, test_years, label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ean_descriptions = ' '.join(lidl_test.ean_name)\n",
    "#all_ean_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "all_words = wordcloud.WordCloud()\n",
    "display(SVG(all_words.generate_from_text(all_ean_descriptions).to_svg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display, Markdown\n",
    "\n",
    "def wordcloud_per_coicop(dataframe: pd.DataFrame, coicop_column: str):\n",
    "    unique_categories = dataframe[coicop_column].unique().tolist()\n",
    "    for unique_category in unique_categories:\n",
    "        category_df = dataframe[dataframe[coicop_column] == unique_category]\n",
    "        category_eans = ' '.join(category_df.ean_name)\n",
    "        \n",
    "        display(Markdown(f\"# {coicop_column}: {unique_category} ({len(category_df.ean_name)} descriptions)\"))\n",
    "        \n",
    "        category_wordcloud = wordcloud.WordCloud()\n",
    "        display(SVG(category_wordcloud.generate_from_text(category_eans).to_svg()))\n",
    "    \n",
    "wordcloud_per_coicop(lidl_test, \"coicop_division\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_per_coicop(lidl_test, \"coicop_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidl_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from typing import Callable, Any\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def cosine_distance_group(column_values: pd.Series, number_of_splits: int = 10, metric: str = \"cosine\") -> float:\n",
    "    column_splits = np.array_split(column_values.values, number_of_splits)\n",
    "    \n",
    "    distance_sums = []\n",
    "    for i in range(len(column_splits)):\n",
    "        for j in range(i, len(column_splits)):\n",
    "            distances = pairwise_distances(column_splits[i].tolist(), column_splits[j].tolist(), metric=metric)\n",
    "            distance_sums.append(distances.sum())\n",
    "    n = (len(column_values) ** 2)    \n",
    "    return np.array(distance_sums).sum() / n\n",
    "\n",
    "def apply_per_group(dataframe: pd.DataFrame, coicop_column: str, group_function: Callable[[pd.DataFrame], Any]) -> pd.DataFrame:\n",
    "    unique_categories = dataframe[coicop_column].unique().tolist()\n",
    "    results_dict = dict()\n",
    "    for unique_category in tqdm.tqdm(unique_categories):\n",
    "        category_df = dataframe[dataframe[coicop_column] == unique_category]\n",
    "        print(category_df.columns)\n",
    "        results_dict[unique_category] = group_function(category_df)\n",
    "    return pd.DataFrame(results_dict, index=[0])\n",
    "        \n",
    "#average_nlp_md = lidl_test.groupby(by=\"coicop_division\").apply(lambda x: cosine_distance_group(x[\"ean_name\"], nlp_md)).reset_index()\n",
    "average_nlp_md = apply_per_group(lidl_test, \"coicop_division\", lambda group_df: cosine_distance_group(group_df[\"features\"]))\n",
    "average_nlp_md    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_group_nlp_md = apply_per_group(lidl_test, \"coicop_group\", lambda group_df: cosine_distance_group(group_df[\"features\"]))\n",
    "average_group_nlp_md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssi",
   "language": "python",
   "name": "ssi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
